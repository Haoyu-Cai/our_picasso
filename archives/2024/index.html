<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Picasso</title><meta name="author" content="Haoyu-Cai"><link rel="shortcut icon" href="/our_picasso/img/favicon.png"><link rel="stylesheet" href="/our_picasso/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/our_picasso/">Picasso</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/our_picasso/images/profile.jpg" onerror="this.onerror=null;this.src='/our_picasso/images/profile.jpg'" alt="avatar"></div><div class="author-discrip"><h3>Haoyu-Cai</h3><p class="author-bio"></p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><article><div class="content">
  <h1>Picasso: Exploring a Parameter Analysis Approach from the Temporal-Spatial Perspective for Text-To-Image Diffusion Models </h1>
  <p id="authors"><a>Haoyu Cai<sup>1</sup></a> &nbsp; &nbsp; <a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~cswang/">Chao Wang<sup>1,2,3</sup></a> &nbsp;&nbsp;  <a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~xhzhou/">Xuehai Zhou<sup>1,3</sup></a> &nbsp;&nbsp; <a target="_blank" rel="noopener" href="http://home.ustc.edu.cn/~louwenqi">Wenqi Lou<sup>3</sup></a><br>

<p>  <span style="font-size: 16px"><sup>1</sup> School of Computer Science, USTC &nbsp;&nbsp;<sup>2</sup> Suzhou Institute for Advanced Research, USTC &nbsp;&nbsp;<sup>3</sup> School of Software Engineering, USTC<br>  </span></p></p>
</div>

<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Visual Concept Implantation (VCI) is essential in text-to-image fields. While VCI methods in diffusion models (VCI in DM) have matured, Visual Concept Disentanglement (VCD) remains an unexplored area. VCD involves analyzing Prompt Spaces trained in VCI to produce disentangled SubPrompts or SubCones. However, challenges arise due to Prompt Space design complexity and feature information extraction. We propose Picasso, a unified framework for VCD in DM. </p>
<p>Our contributions include: </p>
<ul>
<li>for performance evaluation: Transforming VCD in DM into a regular clustering task by Visualization based on SubCones (VbSC); </li>
<li>for a unified framework design: Picasso processes diverse Prompt Spaces using spatially clusterable features from the Scones Set; </li>
<li>for prompt space exploration: Introducing a temporal-spatial SOTA Prompt design subset based on temporal features.</li>
</ul>
<p>Our method provides a feasible mechanism for VCD in DM. Through functional and interpretability validation methods, we will comprehensively evaluate the effectiveness of our proposed method in visual concept implantation tasks and verify the correctness of the parameter space design principles.</p>
<div style="text-align: center;">
<img class="summary-img" src="images/subfig1.jpg" style="width:60%;">
</div>
<br>
<div class="img-legend"><b>Fig.1</b> Temporal-Spatial Parameter Analysis Approach. <i>(a) Visualization Concept Disentanglement Mechanism in Diffusion Models based on Prompt Space Analysis; (b) The Input Extension Formats in Prompt Space Analysis; (c) Evaluation Method bridged by VbSC for Visualization Disentanglement.</i> </div>

<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>In <b>Fig.1</b>(a), we introduce the VCD Mechanism in DM based on Prompt Space analysis. The significance of this research lies in providing a feasible mechanism for VCD in DM tasks. The VCD in the DM problem is nontrivial because of the following complexities:<br>(1) <strong>Prompt Space Design</strong>: The design of the Prompt Space significantly impacts the complexity of parameter analysis. In contrast to general neural network models, one of the main challenges in analyzing the Prompt Space of diffusion models is the temporal inference computation flow. The temporal inference computation flow is in a time-multiplexing manner for the inner parameters of the diffusion model. The time-multiplexing mechanism implies that the same parameter may have vastly different effects at different time steps, leading to less interpretable analysis results. Therefore, introducing temporal elements in the Prompt Space design of diffusion models is necessary. However, introducing temporal elements complicates the analysis process, and high-quality parameter analysis methods must be prioritized.<br>(2) <strong>Parameter Analysis Methods</strong>: Another challenge in analyzing the Prompt Space of diffusion models is feature information extraction. The network structures of conditional generative models like GANs are typically carefully designed, and feature information related to model parameters is often easy to extract or even possesses intrinsic structural features. To reduce convergence costs, designers of diffusion models often use network structures with low interpretability. For example, Stable Diffusion uses UNet as its backbone. Although the CrossAttention layer in diffusion model networks contains critical feature information, existing research indicates that such features are often difficult to analyze directly. Therefore, enhancing the interpretability of feature information through preprocessing is crucial. The search cost of excessively large Prompt Space must also be considered.<br>To address the discussed challenges and limitations, in this work, we propose the first unified and efficient framework Picasso which supports the VCD Mechanism in DM based on Prompt Space analysis and further quantifies the framework’s effectiveness. It is important to lay the foundation for exploring a general solution for VCD. We analyze the performance of various implementation strategies for VCD in DM and its derivative tasks, summarize the empirical rules of Prompt design to improve the performance of the VCI in diffusion models and enhance the feasibility of VCI methods.</p>
<h3 id="VCD-in-DM-Results"><a href="#VCD-in-DM-Results" class="headerlink" title="VCD in DM Results"></a>VCD in DM Results</h3><div style="text-align: center;">
<img class="summary-img" src="images/VCDinDM.jpg" style="width:90%;">
</div>
<br>
<div class="img-legend"><b>Fig.2</b> Our results of VCD in DM. <i> The text enclosed in the blue border represents reference images, while the text enclosed in the yellow border represents
visually presented sub-concepts extracted from the reference images.</i> </div>

<p>By exploring this concept, we aim to gain a deeper understanding of how visual information is processed and represented within these intricate systems. In <b>Fig.1</b>(a), we give an intuitive explanation for VCD in DM mechanism; A single Visual Concept is a broad entity, while an Instance Concept can be further deconstructed into various neuron clusters, termed SubCones (SC). The knowledge about these visual sub-concepts is encapsulated within their corresponding neural regions. Similar to previous work, we can see the visualization results of VCD in DM in <b>Fig.2</b>.<br>In <b>Fig.1</b>(c), by leveraging the SC, we can generate visual results and subsequently embed them into a low-dimensional space using an encoder. The resulting embeddings form distinct groups, which we refer to as Visual Clusters. In the experimental section, we will treat these Visual Clusters as the outcome of a regular clustering task and evaluate them accordingly. The approach allows us to assess the effectiveness of the proposed method in capturing and representing different visual attributes within the neural network’s parameter space.</p>
<h3 id="Clustering-Results-Spatial-Perspective"><a href="#Clustering-Results-Spatial-Perspective" class="headerlink" title="Clustering Results - Spatial Perspective"></a>Clustering Results - Spatial Perspective</h3><div style="text-align: center;">
<img class="summary-img" src="images/cones_identify_a.jpg" style="width:45%;">
</div>
<br>
<div class="img-legend"><b>Fig.3</b> Visual results of FCM clustering. </div>

<p>In practice, we found that too many spatial features were collected during the diffusion model sampling process, making it difficult for the clustering algorithm to converge. According to <strong>Theorem 1</strong>, we introduce a high-weight feature selection strategy based on a simple high-pass filter with <strong>τ</strong> as the threshold. <b>Fig.3</b> demonstrates the visual results of FCM clustering for various values of <strong>τ</strong>, illustrating that the TSCones and their corresponding spatial activation information encapsulate critical insights of the diffusion model about specific concepts. To mitigate the impact of channel dimensionality, we first reduce all spatial activations <strong>F</strong><sub>s</sub> to the same dimensionality using PCA before performing clustering.</p>
<div style="text-align: center;">
<img class="summary-img" src="images/cones_identify_b.jpg" style="width:45%;">
</div>
<br>
<div class="img-legend"><b>Fig.4</b> The relationship between τ , running time, and FPC in a chart. </div>

<p>It is visually evident from the graph that as <strong>τ</strong> increases, the clustering results become more coherent and reasonable, while simultaneously optimizing the time cost required by the clustering algorithm in <b>Fig.4</b>. Therefore, we believe that concept neurons play a pivotal role in the diffusion model’s understanding of specific concepts.</p>
<h3 id="Visualization-Results-Temporal-Perspective"><a href="#Visualization-Results-Temporal-Perspective" class="headerlink" title="Visualization Results - Temporal Perspective"></a>Visualization Results - Temporal Perspective</h3><div style="text-align: center;">
<img class="summary-img" src="images/cov_illustration_l.jpg" style="width:45%;">
</div>
<br>
<div class="img-legend"><b>Fig.5</b> Performance evaluation results of VCD in DM under the Picasso framework. <i>(a) Calinski-Harabasz Index of TSCones with different Temporal Extension Dimensions performing the VCD in DM task; (b) Davies-Bouldin Index of TSCones with different Temporal Extension Dimensions performing the VCD in DM task; (c) Silhouette Coefficient of TSCones with different Temporal Extension Dimensions performing the VCD in DM task. (d)  Calinski-Harabasz Index of TSCones with different cluster numbers Dimensions performing the VCD in DM task. </i> </div>


<p>In <b>Fig.5</b>(b), it confirms our earlier emphasis on temporal locality. However, when there is a significant difference between time steps, their similarity becomes quite low, indicating that the diffusion model does not have temporal globality in a temporal sequence sense. Furthermore, all the diagonal lines exhibit a “broad at the front and narrow at the end” characteristic. It suggests that in the early stages of denoising, neurons corresponding to different denoising steps change their functions very frequently, while in the later stages, which frequency gradually decreases. It is consistent with previous work, early denoising steps are responsible for handling image layout and content, intermediate steps for handling image textures and colors, and final steps for handling image details. It implies that early denoising steps play a more significant role. </p>
<h3 id="Key-Parameter-Ablation"><a href="#Key-Parameter-Ablation" class="headerlink" title="Key Parameter Ablation"></a>Key Parameter Ablation</h3><p>We studied many different parameters and the most key ones are shown here: Temporal Extension Dimension and Cluster Num. The impact of the Temporal Extension Dimension on VCD in DM can provide a reference for designing a parameter space suitable for VCI task. The value of Cluster Num will reveal how many independent visual attributes the diffusion model can usually disentangle when recognizing visual concepts, which is important to recognize how the diffusion model understands visual concepts.</p>
<div style="text-align: center;">
<img class="summary-img" src="images/time_cluster_plot.jpg" style="width:30%;">
</div>
<br>
<div class="img-legend"><b>Fig.6</b> Visual results. <i>(a) Visual result between different time steps. we observe red diagonal lines with a certain width. These diagonal lines with some width suggest that adjacent or closely related diffusion (denoising) steps are responsible for similar functions, and thus their corresponding neural regions are quite similar. (b) Visual result between different sub-concepts. Visualizations of two different scenarios showcase Picasso’s neuromorphic mechanism.</i> </div>

<p>In <b>Fig.6</b>(a) and (b), The db and ch metrics consistently show a performance improvement as the dimensionality is extended from 1 to 3, with the optimal performance achieved at a dimensionality of 3. However, for sc, the optimal values differ across the Avg, Max, and Min scenarios, implying that the data patterns exhibited by the errors differ from those of the averages in <b>Fig.6</b>(c). From this perspective, sc does not appear to be a suitable metric for the VCD in DM task. We speculate that the reason might be that sc is not applicable for evaluating clustering effects on non-convex clusters. In contrast, the clusters generated by the VCD task are complex manifolds, making sc unsuitable for measurement in this context.<br>By employing the elbow method to observe the relationships between ch and Cluster Num, the results indicate that the elbow points occur when Cluster Num is 3 or 4. It implies that the neural network’s intrinsic recognition of images comprises 3-4 visual concepts. Neurobiology also suggests that the human brain decomposes a single visual concept into several sub-visual concepts for cognitive processing, which aligns with our findings. </p>
<h3 id="Single-topic-VCI-Generation"><a href="#Single-topic-VCI-Generation" class="headerlink" title="Single-topic VCI Generation"></a>Single-topic VCI Generation</h3><div style="text-align: center;">
<img class="summary-img" src="images/inversion_sota1.jpg" style="width:40%;">
</div>
<br>
<div class="img-legend"><b>Fig.7</b> Comparisons with SOTA personalization methods including Textual Inversion (TI), DreamBooth, and XTI. </div>

<p>In <b>Fig.7</b>, we show the result of “A tilting cat wearing sunglasse” and “A teddy in Times Square”. XTI and Perfusion are the latest published methods, and the model has not been released yet. The resulting images of XTI and Perfusion are borrowed from their paper, so the results of adding concepts are not shown. Our method is faithful to conveying the appearance and material of the reference image while having better controllability and diversity.</p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2024 by Haoyu-Cai</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/our_picasso/js/main.js"></script></body></html>